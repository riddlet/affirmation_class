{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing word vectors.\n",
      "Found 2196016 word vectors.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "np.random.seed(1337)\n",
    "__docformat__ = 'restructedtext en'\n",
    "import timeit\n",
    "import numpy\n",
    "import scipy.io\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers import Dense, Input, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.models import Model\n",
    "import sys\n",
    "BASE_DIR = '/Users/travis/Documents/Gits'\n",
    "GLOVE_DIR = BASE_DIR + '/Data/'\n",
    "#MAX_SEQUENCE_LENGTH = 210\n",
    "MAX_NB_WORDS = 20000\n",
    "EMBEDDING_DIM = 300\n",
    "VALIDATION_SPLIT = 0.2\n",
    "# first, build index mapping words in the embeddings set\n",
    "# to their embedding vector\n",
    "\n",
    "print('Indexing word vectors.')\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(GLOVE_DIR, 'glove.840B.300d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for custom metrics\n",
    "import keras.backend as K\n",
    "from keras.utils.generic_utils import get_from_module\n",
    "\n",
    "def categorical_accuracy(y_true, y_pred):\n",
    "    \"\"\"Categorical accuracy metric.\n",
    "\n",
    "    Computes the mean accuracy rate across all predictions for\n",
    "    multiclass classification problems.\n",
    "    \"\"\"\n",
    "    return K.mean(K.equal(K.argmax(y_true, axis=-1),\n",
    "                          K.argmax(y_pred, axis=-1)))\n",
    "\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    \"\"\"Precision metric.\n",
    "\n",
    "    Only computes a batch-wise average of precision.\n",
    "\n",
    "    Computes the precision, a metric for multi-label classification of\n",
    "    how many selected items are relevant.\n",
    "    \"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    \"\"\"Recall metric.\n",
    "\n",
    "    Only computes a batch-wise average of recall.\n",
    "\n",
    "    Computes the recall, a metric for multi-label classification of\n",
    "    how many relevant items are selected.\n",
    "    \"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "\n",
    "def fbeta_score(y_true, y_pred, beta=1):\n",
    "    \"\"\"Computes the F score.\n",
    "\n",
    "    The F score is the weighted harmonic mean of precision and recall.\n",
    "    Here it is only computed as a batch-wise average, not globally.\n",
    "\n",
    "    This is useful for multi-label classification, where input samples can be\n",
    "    classified as sets of labels. By only using accuracy (precision) a model\n",
    "    would achieve a perfect score by simply assigning every class to every\n",
    "    input. In order to avoid this, a metric should penalize incorrect class\n",
    "    assignments as well (recall). The F-beta score (ranged from 0.0 to 1.0)\n",
    "    computes this, as a weighted mean of the proportion of correct class\n",
    "    assignments vs. the proportion of incorrect class assignments.\n",
    "\n",
    "    With beta = 1, this is equivalent to a F-measure. With beta < 1, assigning\n",
    "    correct classes becomes more important, and with beta > 1 the metric is\n",
    "    instead weighted towards penalizing incorrect class assignments.\n",
    "    \"\"\"\n",
    "    if beta < 0:\n",
    "        raise ValueError('The lowest choosable beta is zero (only precision).')\n",
    "\n",
    "    # If there are no true positives, fix the F score at 0 like sklearn.\n",
    "    if K.sum(K.round(K.clip(y_true, 0, 1))) == 0:\n",
    "        return 0\n",
    "\n",
    "    p = precision(y_true, y_pred)\n",
    "    r = recall(y_true, y_pred)\n",
    "    bb = beta ** 2\n",
    "    fbeta_score = (1 + bb) * (p * r) / (bb * p + r + K.epsilon())\n",
    "    return fbeta_score\n",
    "\n",
    "\n",
    "def fmeasure(y_true, y_pred):\n",
    "    \"\"\"Computes the f-measure, the harmonic mean of precision and recall.\n",
    "\n",
    "    Here it is only computed as a batch-wise average, not globally.\n",
    "    \"\"\"\n",
    "    return fbeta_score(y_true, y_pred, beta=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_dat(df_ess, df_dem):\n",
    "    df_ess = df_ess[df_ess.Study=='Connecticut']\n",
    "    df_ess.Condition.replace(['c', 'c2', 'c1', 'c3', 'ca', 'cb', '3'], 'Control', inplace=True)\n",
    "    df_ess.Condition.replace(['t', 't2', 't3', 't1', '1', '2', 'ta', 'tb'], 'Treatment', inplace=True)\n",
    "    df_ess.Condition.replace(['c/t'], np.nan, inplace=True)\n",
    "    \n",
    "    df_dem = df_dem[df_dem.Study=='Connecticut']\n",
    "    df_dem.Ethnicity.replace('Asian', 'Asian American', inplace=True)\n",
    "    df_dem.Ethnicity.replace('Other/Mixed', 'Other', inplace=True)\n",
    "    df_dem = df_dem[['ID', 'Ethnicity', 'Gender']].dropna()\n",
    "    \n",
    "    outdat = pd.merge(df_ess[['ID', 'Intervention_number', 'Essay', 'Condition', 'Intervention_Date', 'corrected']], \n",
    "                    df_dem, how='left', on='ID').drop_duplicates()\n",
    "    \n",
    "    return(outdat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here, we're preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing text dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python2.7/site-packages/pandas/core/generic.py:3443: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._update_inplace(new_data)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6224 texts.\n",
      "Found 5816 unique tokens.\n",
      "Shape of data tensor: (6224, 208)\n",
      "Shape of label tensor: (6224, 4)\n",
      "Shape of xtrain tensor: (5291, 208)\n",
      "Shape of ytrain tensor: (5291, 4)\n",
      "Shape of xval tensor: (933, 208)\n",
      "Shape of yval tensor: (933, 4)\n",
      "Preparing embedding matrix.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib\n",
    "from pandas import DataFrame, read_csv\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd #this is how I usually import pandas\n",
    "import sys #only needed to determine Python version number\n",
    "import matplotlib #only needed to determine Matplotlib version number\n",
    "import numpy as np\n",
    "# second, prepare text samples and their labels\n",
    "print('Processing text dataset')\n",
    "df_ess = pd.read_csv('../../Data/3 CSV Files/essays1.23.16.csv', sep='|')\n",
    "df_dem = pd.read_csv('../../Data/3 CSV Files/demog3.2.16.csv')\n",
    "df = create_dat(df_ess, df_dem)\n",
    "df.dropna(axis=0, subset=['corrected', 'Condition'], inplace=True)\n",
    "texts = []  # list of text samples\n",
    "labels_index = {}  # dictionary mapping label name to numeric id\n",
    "labels = []  # list of label ids\n",
    "labels_index['conm']=0\n",
    "labels_index['conf']=1\n",
    "labels_index['affm']=2\n",
    "labels_index['afff']=3\n",
    "texts = df[\"corrected\"].tolist()\n",
    "labelsType = df[\"Condition\"].tolist()\n",
    "labelsRace = df[\"Gender\"].tolist()\n",
    "labelsCombined=[]\n",
    "for i in range(len(labelsType)):\n",
    "    if labelsType[i]=='Control' and labelsRace[i]=='m':\n",
    "        labelsCombined.append(0)\n",
    "    elif labelsType[i]=='Control' and labelsRace[i]=='f':\n",
    "        labelsCombined.append(1)\n",
    "    elif labelsType[i]=='Treatment' and labelsRace[i]=='m':\n",
    "        labelsCombined.append(2)\n",
    "    elif labelsType[i]=='Treatment' and labelsRace[i]=='f':\n",
    "        labelsCombined.append(3)\n",
    "        \n",
    "print('Found %s texts.' % len(texts))\n",
    "\n",
    "# finally, vectorize the text samples into a 2D integer tensor\n",
    "tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "maxseqval=0\n",
    "for i in range(len(sequences)):\n",
    "    if len(sequences[i])>maxseqval:\n",
    "        maxseqval=len(sequences[i])\n",
    "        \n",
    "MAX_SEQUENCE_LENGTH = maxseqval\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "labels = to_categorical(np.asarray(labelsCombined))\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "# split the data into a training set and a validation set\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "x_train = data\n",
    "y_train = labels\n",
    "\n",
    "# split the data into a training set and a validation set\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "VALIDATION_SPLIT = 0.15\n",
    "nb_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "\n",
    "x_train = data[:-nb_validation_samples]\n",
    "y_train = labels[:-nb_validation_samples]\n",
    "x_val = data[-nb_validation_samples:]\n",
    "y_val = labels[-nb_validation_samples:]\n",
    "\n",
    "textstest=[]\n",
    "textsshuff=[]\n",
    "for i in indices:\n",
    "  textsshuff.append(texts[i])\n",
    "index_word={}\n",
    "for i,x in word_index.items():\n",
    "    index_word[x]=i\n",
    "textstest=textsshuff[-nb_validation_samples:-nb_validation_samples]\n",
    "\n",
    "thefile = open('gendercleantimedistmodeltesttexts.txt', 'w')\n",
    "for item in textstest:\n",
    "  thefile.write(\"%s\\n\" % item)  \n",
    "\n",
    "#np.save('gendercleantimedistmodelxtest.npy', x_test)\n",
    "#np.save('gendercleantimedistmodelytest.npy', y_test)\n",
    "\n",
    "print('Shape of xtrain tensor:', x_train.shape)\n",
    "print('Shape of ytrain tensor:', y_train.shape)\n",
    "print('Shape of xval tensor:', x_val.shape)\n",
    "print('Shape of yval tensor:', y_val.shape)\n",
    "#print('Shape of xtest tensor:', x_test.shape)\n",
    "#print('Shape of ytest tensor:', y_test.shape)\n",
    "\n",
    "print('Preparing embedding matrix.')\n",
    "# prepare embedding matrix\n",
    "nb_words = min(MAX_NB_WORDS, len(word_index))\n",
    "embedding_matrix = np.zeros((nb_words + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i > MAX_NB_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "# load pre-trained word embeddings into an Embedding layer\n",
    "# note that we set trainable = False so as to keep the embeddings fixed\n",
    "embedding_layer = Embedding(nb_words + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next, we define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_3 (Embedding)          (None, 208, 300)      1745100     embedding_input_2[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)              (None, 208, 300)      0           embedding_3[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                    (None, 50)            70200       dropout_4[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)              (None, 50)            0           lstm_3[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, 4)             204         dropout_5[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 1,815,504\n",
      "Trainable params: 1,815,504\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import load_model\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(nb_words + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=True,dropout=0.2))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(LSTM(50))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(len(labels_index), activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit the model\n",
    "\n",
    "I'm fitting it to all the data here, so as to make predictions using novel sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_2 (Embedding)          (None, 208, 300)      1745100     embedding_input_1[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)              (None, 208, 300)      0           embedding_2[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                    (None, 50)            70200       dropout_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)              (None, 50)            0           lstm_1[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 4)             204         dropout_2[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 1,815,504\n",
      "Trainable params: 1,815,504\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "None\n",
      "Train on 5291 samples, validate on 933 samples\n",
      "Epoch 1/35\n",
      "5291/5291 [==============================] - 44s - loss: 1.2205 - precision: 0.5473 - recall: 0.1028 - fmeasure: 0.1558 - categorical_accuracy: 0.3937 - val_loss: 1.1688 - val_precision: 0.4888 - val_recall: 0.3365 - val_fmeasure: 0.3982 - val_categorical_accuracy: 0.4630\n",
      "Epoch 2/35\n",
      "5291/5291 [==============================] - 44s - loss: 0.9888 - precision: 0.5370 - recall: 0.2992 - fmeasure: 0.3819 - categorical_accuracy: 0.4952 - val_loss: 0.8783 - val_precision: 0.5622 - val_recall: 0.4770 - val_fmeasure: 0.5157 - val_categorical_accuracy: 0.5456\n",
      "Epoch 3/35\n",
      "5291/5291 [==============================] - 44s - loss: 0.9213 - precision: 0.5618 - recall: 0.3958 - fmeasure: 0.4638 - categorical_accuracy: 0.5252 - val_loss: 0.8177 - val_precision: 0.5904 - val_recall: 0.5016 - val_fmeasure: 0.5421 - val_categorical_accuracy: 0.5723\n",
      "Epoch 4/35\n",
      "5291/5291 [==============================] - 44s - loss: 0.8765 - precision: 0.5692 - recall: 0.4328 - fmeasure: 0.4912 - categorical_accuracy: 0.5356 - val_loss: 0.7499 - val_precision: 0.6153 - val_recall: 0.5488 - val_fmeasure: 0.5799 - val_categorical_accuracy: 0.5841\n",
      "Epoch 5/35\n",
      "5291/5291 [==============================] - 44s - loss: 0.8787 - precision: 0.5812 - recall: 0.4525 - fmeasure: 0.5084 - categorical_accuracy: 0.5487 - val_loss: 0.8127 - val_precision: 0.6103 - val_recall: 0.5702 - val_fmeasure: 0.5894 - val_categorical_accuracy: 0.5938\n",
      "Epoch 6/35\n",
      "5291/5291 [==============================] - 44s - loss: 0.8312 - precision: 0.5971 - recall: 0.4893 - fmeasure: 0.5376 - categorical_accuracy: 0.5683 - val_loss: 0.7401 - val_precision: 0.6034 - val_recall: 0.5595 - val_fmeasure: 0.5806 - val_categorical_accuracy: 0.5863\n",
      "Epoch 7/35\n",
      "5291/5291 [==============================] - 45s - loss: 0.8057 - precision: 0.6095 - recall: 0.5056 - fmeasure: 0.5523 - categorical_accuracy: 0.5780 - val_loss: 0.7618 - val_precision: 0.6140 - val_recall: 0.5831 - val_fmeasure: 0.5980 - val_categorical_accuracy: 0.6002\n",
      "Epoch 8/35\n",
      "5291/5291 [==============================] - 44s - loss: 0.7927 - precision: 0.6272 - recall: 0.5377 - fmeasure: 0.5788 - categorical_accuracy: 0.6018 - val_loss: 0.7417 - val_precision: 0.6123 - val_recall: 0.5702 - val_fmeasure: 0.5903 - val_categorical_accuracy: 0.6013\n",
      "Epoch 9/35\n",
      "5291/5291 [==============================] - 44s - loss: 0.7741 - precision: 0.6255 - recall: 0.5369 - fmeasure: 0.5775 - categorical_accuracy: 0.5991 - val_loss: 0.7328 - val_precision: 0.6342 - val_recall: 0.6077 - val_fmeasure: 0.6206 - val_categorical_accuracy: 0.6249\n",
      "Epoch 10/35\n",
      "5291/5291 [==============================] - 43s - loss: 0.7777 - precision: 0.6324 - recall: 0.5528 - fmeasure: 0.5896 - categorical_accuracy: 0.6069 - val_loss: 0.7416 - val_precision: 0.6352 - val_recall: 0.6034 - val_fmeasure: 0.6188 - val_categorical_accuracy: 0.6195\n",
      "Epoch 11/35\n",
      "5291/5291 [==============================] - 44s - loss: 0.7614 - precision: 0.6439 - recall: 0.5651 - fmeasure: 0.6017 - categorical_accuracy: 0.6220 - val_loss: 0.7181 - val_precision: 0.6385 - val_recall: 0.6163 - val_fmeasure: 0.6272 - val_categorical_accuracy: 0.6281\n",
      "Epoch 12/35\n",
      "5291/5291 [==============================] - 44s - loss: 0.7399 - precision: 0.6662 - recall: 0.5912 - fmeasure: 0.6262 - categorical_accuracy: 0.6401 - val_loss: 0.6879 - val_precision: 0.6484 - val_recall: 0.6195 - val_fmeasure: 0.6335 - val_categorical_accuracy: 0.6388\n",
      "Epoch 13/35\n",
      "5291/5291 [==============================] - 44s - loss: 0.7297 - precision: 0.6664 - recall: 0.5965 - fmeasure: 0.6293 - categorical_accuracy: 0.6417 - val_loss: 0.6886 - val_precision: 0.6537 - val_recall: 0.6270 - val_fmeasure: 0.6399 - val_categorical_accuracy: 0.6474\n",
      "Epoch 14/35\n",
      "5291/5291 [==============================] - 44s - loss: 0.7162 - precision: 0.6825 - recall: 0.6127 - fmeasure: 0.6456 - categorical_accuracy: 0.6602 - val_loss: 0.8259 - val_precision: 0.6106 - val_recall: 0.5884 - val_fmeasure: 0.5993 - val_categorical_accuracy: 0.6045\n",
      "Epoch 15/35\n",
      "5291/5291 [==============================] - 44s - loss: 0.7152 - precision: 0.6722 - recall: 0.6069 - fmeasure: 0.6377 - categorical_accuracy: 0.6490 - val_loss: 0.7230 - val_precision: 0.6215 - val_recall: 0.6013 - val_fmeasure: 0.6111 - val_categorical_accuracy: 0.6120\n",
      "Epoch 16/35\n",
      "5291/5291 [==============================] - 43s - loss: 0.6876 - precision: 0.6947 - recall: 0.6332 - fmeasure: 0.6624 - categorical_accuracy: 0.6702 - val_loss: 0.7124 - val_precision: 0.6509 - val_recall: 0.6259 - val_fmeasure: 0.6381 - val_categorical_accuracy: 0.6420\n",
      "Epoch 17/35\n",
      "5291/5291 [==============================] - 44s - loss: 0.6794 - precision: 0.7000 - recall: 0.6352 - fmeasure: 0.6659 - categorical_accuracy: 0.6793 - val_loss: 0.7497 - val_precision: 0.6517 - val_recall: 0.6281 - val_fmeasure: 0.6397 - val_categorical_accuracy: 0.6409\n",
      "Epoch 18/35\n",
      "5291/5291 [==============================] - 43s - loss: 0.6775 - precision: 0.7036 - recall: 0.6449 - fmeasure: 0.6728 - categorical_accuracy: 0.6838 - val_loss: 0.7100 - val_precision: 0.6619 - val_recall: 0.6409 - val_fmeasure: 0.6511 - val_categorical_accuracy: 0.6581\n",
      "Epoch 19/35\n",
      "5291/5291 [==============================] - 44s - loss: 0.6629 - precision: 0.7069 - recall: 0.6534 - fmeasure: 0.6789 - categorical_accuracy: 0.6883 - val_loss: 0.7658 - val_precision: 0.6470 - val_recall: 0.6238 - val_fmeasure: 0.6351 - val_categorical_accuracy: 0.6388\n",
      "Epoch 20/35\n",
      "5291/5291 [==============================] - 44s - loss: 0.6541 - precision: 0.7131 - recall: 0.6594 - fmeasure: 0.6851 - categorical_accuracy: 0.6948 - val_loss: 0.6970 - val_precision: 0.6565 - val_recall: 0.6324 - val_fmeasure: 0.6441 - val_categorical_accuracy: 0.6484\n",
      "Epoch 21/35\n",
      "5291/5291 [==============================] - 43s - loss: 0.6441 - precision: 0.7061 - recall: 0.6572 - fmeasure: 0.6806 - categorical_accuracy: 0.6868 - val_loss: 0.7211 - val_precision: 0.6440 - val_recall: 0.6302 - val_fmeasure: 0.6370 - val_categorical_accuracy: 0.6409\n",
      "Epoch 22/35\n",
      "5291/5291 [==============================] - 43s - loss: 0.6303 - precision: 0.7257 - recall: 0.6832 - fmeasure: 0.7037 - categorical_accuracy: 0.7091 - val_loss: 0.7910 - val_precision: 0.6025 - val_recall: 0.5884 - val_fmeasure: 0.5954 - val_categorical_accuracy: 0.5970\n",
      "Epoch 23/35\n",
      "5291/5291 [==============================] - 44s - loss: 0.6228 - precision: 0.7307 - recall: 0.6881 - fmeasure: 0.7087 - categorical_accuracy: 0.7118 - val_loss: 0.7005 - val_precision: 0.6575 - val_recall: 0.6420 - val_fmeasure: 0.6496 - val_categorical_accuracy: 0.6538\n",
      "Epoch 24/35\n",
      "5291/5291 [==============================] - 44s - loss: 0.6133 - precision: 0.7278 - recall: 0.6874 - fmeasure: 0.7069 - categorical_accuracy: 0.7123 - val_loss: 0.7861 - val_precision: 0.5906 - val_recall: 0.5809 - val_fmeasure: 0.5857 - val_categorical_accuracy: 0.5884\n",
      "Epoch 25/35\n",
      "5291/5291 [==============================] - 44s - loss: 0.6282 - precision: 0.7316 - recall: 0.6846 - fmeasure: 0.7072 - categorical_accuracy: 0.7157 - val_loss: 0.7304 - val_precision: 0.6481 - val_recall: 0.6313 - val_fmeasure: 0.6396 - val_categorical_accuracy: 0.6409\n",
      "Epoch 26/35\n",
      "5291/5291 [==============================] - 44s - loss: 0.5821 - precision: 0.7507 - recall: 0.7114 - fmeasure: 0.7304 - categorical_accuracy: 0.7356 - val_loss: 0.7507 - val_precision: 0.6653 - val_recall: 0.6474 - val_fmeasure: 0.6561 - val_categorical_accuracy: 0.6559\n",
      "Epoch 27/35\n",
      "5291/5291 [==============================] - 44s - loss: 0.5894 - precision: 0.7449 - recall: 0.7006 - fmeasure: 0.7219 - categorical_accuracy: 0.7286 - val_loss: 0.7516 - val_precision: 0.6407 - val_recall: 0.6259 - val_fmeasure: 0.6332 - val_categorical_accuracy: 0.6313\n",
      "Epoch 28/35\n",
      "5291/5291 [==============================] - 44s - loss: 0.5780 - precision: 0.7536 - recall: 0.7161 - fmeasure: 0.7343 - categorical_accuracy: 0.7365 - val_loss: 0.7558 - val_precision: 0.6361 - val_recall: 0.6259 - val_fmeasure: 0.6310 - val_categorical_accuracy: 0.6313\n",
      "Epoch 29/35\n",
      "5291/5291 [==============================] - 43s - loss: 0.5754 - precision: 0.7565 - recall: 0.7167 - fmeasure: 0.7359 - categorical_accuracy: 0.7407 - val_loss: 0.7531 - val_precision: 0.6375 - val_recall: 0.6238 - val_fmeasure: 0.6305 - val_categorical_accuracy: 0.6324\n",
      "Epoch 30/35\n",
      "5291/5291 [==============================] - 44s - loss: 0.5617 - precision: 0.7589 - recall: 0.7239 - fmeasure: 0.7409 - categorical_accuracy: 0.7437 - val_loss: 0.7689 - val_precision: 0.6496 - val_recall: 0.6334 - val_fmeasure: 0.6414 - val_categorical_accuracy: 0.6409\n",
      "Epoch 31/35\n",
      "5291/5291 [==============================] - 44s - loss: 0.5508 - precision: 0.7700 - recall: 0.7318 - fmeasure: 0.7503 - categorical_accuracy: 0.7513 - val_loss: 0.8189 - val_precision: 0.6248 - val_recall: 0.6131 - val_fmeasure: 0.6188 - val_categorical_accuracy: 0.6195\n",
      "Epoch 32/35\n",
      "5291/5291 [==============================] - 44s - loss: 0.5427 - precision: 0.7675 - recall: 0.7329 - fmeasure: 0.7497 - categorical_accuracy: 0.7537 - val_loss: 0.8172 - val_precision: 0.6299 - val_recall: 0.6184 - val_fmeasure: 0.6241 - val_categorical_accuracy: 0.6238\n",
      "Epoch 33/35\n",
      "5291/5291 [==============================] - 44s - loss: 0.5333 - precision: 0.7868 - recall: 0.7518 - fmeasure: 0.7688 - categorical_accuracy: 0.7709 - val_loss: 0.8078 - val_precision: 0.6541 - val_recall: 0.6409 - val_fmeasure: 0.6474 - val_categorical_accuracy: 0.6463\n",
      "Epoch 34/35\n",
      "5291/5291 [==============================] - 44s - loss: 0.5292 - precision: 0.7762 - recall: 0.7473 - fmeasure: 0.7614 - categorical_accuracy: 0.7630 - val_loss: 0.7987 - val_precision: 0.6406 - val_recall: 0.6302 - val_fmeasure: 0.6353 - val_categorical_accuracy: 0.6356\n",
      "Epoch 35/35\n",
      "5291/5291 [==============================] - 43s - loss: 0.5355 - precision: 0.7822 - recall: 0.7481 - fmeasure: 0.7647 - categorical_accuracy: 0.7681 - val_loss: 0.8261 - val_precision: 0.6403 - val_recall: 0.6324 - val_fmeasure: 0.6363 - val_categorical_accuracy: 0.6345\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=[precision,recall,fmeasure,categorical_accuracy])\n",
    "print(model.summary())\n",
    "bz=128\n",
    "model.fit(x_train, y_train, validation_data=(x_val, y_val),\n",
    "          nb_epoch=35, batch_size=bz)\n",
    "model.save_weights('finalgendermodel.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next, restructure the model to make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_3 (Embedding)          (None, 208, 300)      1745100     embedding_input_2[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)              (None, 208, 300)      0           embedding_3[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                    (None, 50)            70200       dropout_4[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)              (None, 50)            0           lstm_3[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, 4)             204         dropout_5[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 1,815,504\n",
      "Trainable params: 1,815,504\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['dense_3', 'lstm_3', 'dropout_5', 'dropout_4', 'embedding_3']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import TimeDistributed\n",
    "model.load_weights('finalgendermodel.h5')\n",
    "model.summary()\n",
    "layer_dict = dict([(layer.name, layer) for layer in model.layers])\n",
    "layer_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_3 (Embedding)          (None, 208, 300)      1745100     embedding_input_2[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)              (None, 208, 300)      0           embedding_3[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "lstm_4 (LSTM)                    (None, 208, 50)       70200       dropout_4[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)              (None, 208, 50)       0           lstm_4[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "timedistributed_2 (TimeDistribut (None, 208, 4)        204         dropout_6[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 1,815,504\n",
      "Trainable params: 1,815,504\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#layer_dict['lstm_2'].get_weights()\n",
    "dens=layer_dict['dense_3'].get_weights()\n",
    "lstmw=layer_dict['lstm_3'].get_weights()\n",
    "model.pop()\n",
    "model.pop()\n",
    "model.pop()\n",
    "lstmout = LSTM(50,\n",
    "               return_sequences=True,\n",
    "               stateful=False,weights=lstmw)\n",
    "model.add(lstmout)\n",
    "model.add(Dropout(0.5))\n",
    "templayer=TimeDistributed(Dense(len(labels_index), activation='softmax',weights=dens))\n",
    "model.add(templayer)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here, I make some novel sentences to pass to the network\n",
    "(social words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../liwcpy/liwc-dict/Social.csv', header=None, names=['word'], encoding='UTF-8')\n",
    "df = df[~df.word.str.contains('\\*')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['pos'] = df['word'].apply(lambda x: nlp(x)[0].pos_)\n",
    "df['lemma'] = df['word'].apply(lambda x: nlp(x)[0].lemma_)\n",
    "\n",
    "df['sent'] = 'These values are important because my'\n",
    "df.sent.loc[df.pos=='VERB'] = 'These values'\n",
    "df.sent.loc[df.pos=='PRON'] = 'These values are important to'\n",
    "df.sent.loc[df.pos=='ADJ'] = 'These values are important because my'\n",
    "df.sent.loc[df.pos=='INTJ'] = ''\n",
    "df.sent.loc[df.pos=='ADV'] = 'These values are important because my'\n",
    "df['test_sent'] = df.sent+' '+df.word\n",
    "df['sent_wo'] = 'These values are important because without my'\n",
    "df.sent_wo.loc[df.pos=='VERB'] = 'These values don\\'t' \n",
    "df.sent_wo.loc[df.pos=='PRON'] = 'These values are not important to'\n",
    "df.sent_wo.loc[df.pos=='ADJ'] = 'These values are important because without my'\n",
    "df.sent_wo.loc[df.pos=='INTJ'] = ''\n",
    "df.sent_wo.loc[df.pos=='ADV'] = 'These values are important because without my'\n",
    "df['test_sent_wo'] = df.sent_wo+' '+df.lemma\n",
    "df.test_sent_wo.loc[df.pos=='PRON'] = df.sent_wo + ' ' + df.word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make predictions and save important bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "259/259 [==============================] - 1s     \n",
      "259/259 [==============================] - 0s     \n"
     ]
    }
   ],
   "source": [
    "subdat = df.drop_duplicates(subset='test_sent')\n",
    "subdat['test_sent'] = subdat['test_sent'].apply(lambda x: str(x))\n",
    "test_seq = tokenizer.texts_to_sequences(subdat['test_sent'])\n",
    "full_dat = pad_sequences(test_seq, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "preds = model.predict(full_dat, verbose=1)\n",
    "word_index = tokenizer.word_index\n",
    "index_word={}\n",
    "for i,x in word_index.items():\n",
    "    index_word[x]=i\n",
    "np.save('testsentiw_soc.npy', index_word)\n",
    "np.save('testsentiw_inv_soc.npy', word_index)\n",
    "np.save('testsentpreds_soc.npy', preds)\n",
    "np.save('testsentxdat_soc.npy', full_dat)\n",
    "np.save('testdat_soc.npy', subdat)\n",
    "\n",
    "subdat['test_sent_wo'] = subdat['test_sent_wo'].apply(lambda x: str(x))\n",
    "test_seq = tokenizer.texts_to_sequences(subdat['test_sent_wo'])\n",
    "full_dat = pad_sequences(test_seq, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "preds_wo = model.predict(full_dat, verbose=1)\n",
    "word_index_wo = tokenizer.word_index\n",
    "index_word_wo={}\n",
    "for i,x in word_index_wo.items():\n",
    "    index_word_wo[x]=i\n",
    "np.save('testsentiw_wo_soc.npy', index_word_wo)\n",
    "np.save('testsentiw_inv_wo_soc.npy', word_index_wo)\n",
    "np.save('testsentpreds_wo_soc.npy', preds_wo)\n",
    "np.save('testsentxdat_wo_soc.npy', full_dat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here, I make some novel sentences to pass to the network\n",
    "(positive emotion words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../liwcpy/liwc-dict/Posemo.csv', header=None, names=['word'], encoding='UTF-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = df[~df.word.str.contains('\\*')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#df[df.word.str.contains('\\*')]\n",
    "df['pos'] = df['word'].apply(lambda x: nlp(x)[0].pos_)\n",
    "df['lemma'] = df['word'].apply(lambda x: nlp(x)[0].lemma_)\n",
    "\n",
    "df['sent'] = 'These values are important because my'\n",
    "df.sent.loc[df.pos=='VERB'] = 'These values'\n",
    "df.sent.loc[df.pos=='PRON'] = 'These values are important to'\n",
    "df.sent.loc[df.pos=='ADJ'] = 'These values are important because my'\n",
    "df.sent.loc[df.pos=='INTJ'] = ''\n",
    "df.sent.loc[df.pos=='ADV'] = 'These values are important because my'\n",
    "df['test_sent'] = df.sent+' '+df.word\n",
    "df['sent_wo'] = 'These values are important because without my'\n",
    "df.sent_wo.loc[df.pos=='VERB'] = 'These values don\\'t' \n",
    "df.sent_wo.loc[df.pos=='PRON'] = 'These values are not important to'\n",
    "df.sent_wo.loc[df.pos=='ADJ'] = 'These values are important because without my'\n",
    "df.sent_wo.loc[df.pos=='INTJ'] = ''\n",
    "df.sent_wo.loc[df.pos=='ADV'] = 'These values are important because without my'\n",
    "df['test_sent_wo'] = df.sent_wo+' '+df.lemma\n",
    "df.test_sent_wo.loc[df.pos=='PRON'] = df.sent_wo + ' ' + df.word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make predictions and save important bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159/159 [==============================] - 0s     \n",
      "159/159 [==============================] - 0s     \n"
     ]
    }
   ],
   "source": [
    "subdat = df.drop_duplicates(subset='test_sent')\n",
    "subdat['test_sent'] = subdat['test_sent'].apply(lambda x: str(x))\n",
    "test_seq = tokenizer.texts_to_sequences(subdat['test_sent'])\n",
    "full_dat = pad_sequences(test_seq, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "preds = model.predict(full_dat, verbose=1)\n",
    "word_index = tokenizer.word_index\n",
    "index_word={}\n",
    "for i,x in word_index.items():\n",
    "    index_word[x]=i\n",
    "np.save('testsentiw_posem.npy', index_word)\n",
    "np.save('testsentiw_inv_posem.npy', word_index)\n",
    "np.save('testsentpreds_posem.npy', preds)\n",
    "np.save('testsentxdat_posem.npy', full_dat)\n",
    "np.save('testdat_posem.npy', subdat)\n",
    "\n",
    "subdat['test_sent_wo'] = subdat['test_sent_wo'].apply(lambda x: str(x))\n",
    "test_seq = tokenizer.texts_to_sequences(subdat['test_sent_wo'])\n",
    "full_dat = pad_sequences(test_seq, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "preds_wo = model.predict(full_dat, verbose=1)\n",
    "word_index_wo = tokenizer.word_index\n",
    "index_word_wo={}\n",
    "for i,x in word_index_wo.items():\n",
    "    index_word_wo[x]=i\n",
    "np.save('testsentiw_wo_posem.npy', index_word_wo)\n",
    "np.save('testsentiw_inv_wo_posem.npy', word_index_wo)\n",
    "np.save('testsentpreds_wo_posem.npy', preds_wo)\n",
    "np.save('testsentxdat_wo_posem.npy', full_dat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here, I make some novel sentences to pass to the network\n",
    "(Negative emotion words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../liwcpy/liwc-dict/Negemo.csv', header=None, names=['word'], encoding='UTF-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = df[~df.word.str.contains('\\*')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#df[df.word.str.contains('\\*')]\n",
    "df['pos'] = df['word'].apply(lambda x: nlp(x)[0].pos_)\n",
    "df['lemma'] = df['word'].apply(lambda x: nlp(x)[0].lemma_)\n",
    "\n",
    "df['sent'] = 'These values are important because my'\n",
    "df.sent.loc[df.pos=='VERB'] = 'These values'\n",
    "df.sent.loc[df.pos=='PRON'] = 'These values are important to'\n",
    "df.sent.loc[df.pos=='ADJ'] = 'These values are important because my'\n",
    "df.sent.loc[df.pos=='INTJ'] = ''\n",
    "df.sent.loc[df.pos=='ADV'] = 'These values are important because my'\n",
    "df['test_sent'] = df.sent+' '+df.word\n",
    "df['sent_wo'] = 'These values are important because without my'\n",
    "df.sent_wo.loc[df.pos=='VERB'] = 'These values don\\'t' \n",
    "df.sent_wo.loc[df.pos=='PRON'] = 'These values are not important to'\n",
    "df.sent_wo.loc[df.pos=='ADJ'] = 'These values are important because without my'\n",
    "df.sent_wo.loc[df.pos=='INTJ'] = ''\n",
    "df.sent_wo.loc[df.pos=='ADV'] = 'These values are important because without my'\n",
    "df['test_sent_wo'] = df.sent_wo+' '+df.lemma\n",
    "df.test_sent_wo.loc[df.pos=='PRON'] = df.sent_wo + ' ' + df.word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make predictions and save important bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150/150 [==============================] - 0s     \n",
      "150/150 [==============================] - 0s     \n"
     ]
    }
   ],
   "source": [
    "subdat = df.drop_duplicates(subset='test_sent')\n",
    "subdat['test_sent'] = subdat['test_sent'].apply(lambda x: str(x))\n",
    "test_seq = tokenizer.texts_to_sequences(subdat['test_sent'])\n",
    "full_dat = pad_sequences(test_seq, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "preds = model.predict(full_dat, verbose=1)\n",
    "word_index = tokenizer.word_index\n",
    "index_word={}\n",
    "for i,x in word_index.items():\n",
    "    index_word[x]=i\n",
    "np.save('testsentiw_negem.npy', index_word)\n",
    "np.save('testsentiw_inv_negem.npy', word_index)\n",
    "np.save('testsentpreds_negem.npy', preds)\n",
    "np.save('testsentxdat_negem.npy', full_dat)\n",
    "np.save('testdat_negem.npy', subdat)\n",
    "\n",
    "subdat['test_sent_wo'] = subdat['test_sent_wo'].apply(lambda x: str(x))\n",
    "test_seq = tokenizer.texts_to_sequences(subdat['test_sent_wo'])\n",
    "full_dat = pad_sequences(test_seq, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "preds_wo = model.predict(full_dat, verbose=1)\n",
    "word_index_wo = tokenizer.word_index\n",
    "index_word_wo={}\n",
    "for i,x in word_index_wo.items():\n",
    "    index_word_wo[x]=i\n",
    "np.save('testsentiw_wo_negem.npy', index_word_wo)\n",
    "np.save('testsentiw_inv_wo_negem.npy', word_index_wo)\n",
    "np.save('testsentpreds_wo_negem.npy', preds_wo)\n",
    "np.save('testsentxdat_wo_negem.npy', full_dat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here, I make some novel sentences to pass to the network\n",
    "(Value-specific sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "texts = ['Athletic ability is important to me', \n",
    "         'Art is important to me', \n",
    "         'Being smart is important to me', \n",
    "         'Getting good grades is important to me', \n",
    "         'Creativity is important to me', \n",
    "         'Independence is important to me', \n",
    "         'Social groups are important to me', \n",
    "         'Music is important to me', \n",
    "         'Politics is important to me', \n",
    "         'Relationships is important to me', \n",
    "         'Religion is important to me', \n",
    "         'Sense of humor is important to me', \n",
    "         'Living in the moment is important to me']\n",
    "\n",
    "val = ['Athletics', 'Art', 'Being Smart', 'Good Grades', 'Creativity', \n",
    "      'Independence', 'Social Groups', 'Music', 'Politics', 'Relationships',\n",
    "      'Religion', 'Sense of Humor', 'Living in the Moment']\n",
    "\n",
    "texts_not = ['Athletic ability is not important to me', \n",
    "         'Art is not important to me', \n",
    "         'Being smart is not important to me', \n",
    "         'Getting good grades is not important to me', \n",
    "         'Creativity is not important to me', \n",
    "         'Independence is not important to me', \n",
    "         'Social groups are not important to me', \n",
    "         'Music is not important to me', \n",
    "         'Politics is not important to me', \n",
    "         'Relationships is not important to me', \n",
    "         'Religion is not important to me', \n",
    "         'Sense of humor is not important to me', \n",
    "         'Living in the moment is not important to me']\n",
    "\n",
    "df = pd.DataFrame({'text':texts,\n",
    "                  'value':val})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make predictions and save important bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 0s\n",
      "13/13 [==============================] - 0s\n"
     ]
    }
   ],
   "source": [
    "test_seq = tokenizer.texts_to_sequences(texts)\n",
    "full_dat = pad_sequences(test_seq, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "preds = model.predict(full_dat, verbose=1)\n",
    "word_index = tokenizer.word_index\n",
    "index_word={}\n",
    "for i,x in word_index.items():\n",
    "    index_word[x]=i\n",
    "np.save('testsentiw_val.npy', index_word)\n",
    "np.save('testsentiw_inv_val.npy', word_index)\n",
    "np.save('testsentpreds_val.npy', preds)\n",
    "np.save('testsentxdat_val.npy', full_dat)\n",
    "np.save('testdat_val.npy', df)\n",
    "\n",
    "test_seq = tokenizer.texts_to_sequences(texts_not)\n",
    "full_dat = pad_sequences(test_seq, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "preds_not = model.predict(full_dat, verbose=1)\n",
    "word_index_not = tokenizer.word_index\n",
    "index_word_not={}\n",
    "for i,x in word_index_not.items():\n",
    "    index_word_not[x]=i\n",
    "np.save('testsentiw_wo_val.npy', index_word_not)\n",
    "np.save('testsentiw_inv_wo_val.npy', word_index_not)\n",
    "np.save('testsentpreds_wo_val.npy', preds_not)\n",
    "np.save('testsentxdat_wo_val.npy', full_dat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 0s\n"
     ]
    }
   ],
   "source": [
    "texts = ['Athletic ability is important to me', \n",
    "         'Art is important to me', \n",
    "         'Being smart is important to me', \n",
    "         'Getting good grades is important to me', \n",
    "         'Creativity is important to me', \n",
    "         'Independence is important to me', \n",
    "         'Social groups are important to me', \n",
    "         'Music is important to me', \n",
    "         'Politics is important to me', \n",
    "         'Relationships is important to me', \n",
    "         'Religion is important to me', \n",
    "         'Sense of humor is important to me', \n",
    "         'Living in the moment is important to me']\n",
    "test_seq = tokenizer.texts_to_sequences(texts)\n",
    "full_dat = pad_sequences(test_seq, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "preds = model.predict(full_dat, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here, I make some novel sentences to pass to the network\n",
    "(Justification with relationships)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>justification</th>\n",
       "      <th>text</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>friends</td>\n",
       "      <td>Athletic ability is important to me because my...</td>\n",
       "      <td>Athletic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>family</td>\n",
       "      <td>Athletic ability is important to me because my...</td>\n",
       "      <td>Athletic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>future</td>\n",
       "      <td>Athletic ability is important to me because my...</td>\n",
       "      <td>Athletic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>parents</td>\n",
       "      <td>Athletic ability is important to me because my...</td>\n",
       "      <td>Athletic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>teacher</td>\n",
       "      <td>Athletic ability is important to me because my...</td>\n",
       "      <td>Athletic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  justification                                               text     value\n",
       "0       friends  Athletic ability is important to me because my...  Athletic\n",
       "1        family  Athletic ability is important to me because my...  Athletic\n",
       "2        future  Athletic ability is important to me because my...  Athletic\n",
       "3       parents  Athletic ability is important to me because my...  Athletic\n",
       "4       teacher  Athletic ability is important to me because my...  Athletic"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import itertools\n",
    "texts = ['Athletic ability is important to me because ', \n",
    "         'Art is important to me because ', \n",
    "         'Being smart is important to me because ', \n",
    "         'Getting good grades is important to me because ', \n",
    "         'Creativity is important to me because ', \n",
    "         'Independence is important to me because ', \n",
    "         'Social groups are important to me because ', \n",
    "         'Music is important to me because ', \n",
    "         'Politics is important to me because ', \n",
    "         'Relationships is important to me because ', \n",
    "         'Religion is important to me because ', \n",
    "         'Sense of humor is important to me because ', \n",
    "         'Living in the moment is important to me because ']\n",
    "\n",
    "justification = ['my friends', 'my family', 'my future', 'my parents', 'my teacher', \n",
    "                 'my friend', 'my dad', 'my mom', 'my sister', 'my brother', \n",
    "                 'my mother', 'my father', 'my cousin', 'my aunt', 'my uncle']\n",
    "\n",
    "\n",
    "a = list(itertools.product(texts, justification))\n",
    "combos = []\n",
    "value = []\n",
    "just = []\n",
    "for i in a:\n",
    "    combos.append(i[0]+i[1])\n",
    "    value.append(i[0].split()[0])\n",
    "    just.append(i[1].split()[1])\n",
    "df = pd.DataFrame({'text':combos,\n",
    "                  'value':value,\n",
    "                  'justification':just})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make predictions and save important bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "195/195 [==============================] - 1s     \n"
     ]
    }
   ],
   "source": [
    "test_seq = tokenizer.texts_to_sequences(combos)\n",
    "full_dat = pad_sequences(test_seq, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "preds = model.predict(full_dat, verbose=1)\n",
    "word_index = tokenizer.word_index\n",
    "index_word={}\n",
    "for i,x in word_index.items():\n",
    "    index_word[x]=i\n",
    "np.save('testsentiw_just.npy', index_word)\n",
    "np.save('testsentiw_inv_just.npy', word_index)\n",
    "np.save('testsentpreds_just.npy', preds)\n",
    "np.save('testsentxdat_just.npy', full_dat)\n",
    "np.save('testdat_just.npy', df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here, I make some novel sentences to pass to the network\n",
    "(Justification with the self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "texts = ['Athletic ability is important to me because ', \n",
    "         'Art is important to me because ', \n",
    "         'Being smart is important to me because ', \n",
    "         'Getting good grades is important to me because ', \n",
    "         'Creativity is important to me because ', \n",
    "         'Independence is important to me because ', \n",
    "         'Social groups are important to me because ', \n",
    "         'Music is important to me because ', \n",
    "         'Politics is important to me because ', \n",
    "         'Relationships is important to me because ', \n",
    "         'Religion is important to me because ', \n",
    "         'Sense of humor is important to me because ', \n",
    "         'Living in the moment is important to me because ']\n",
    "\n",
    "justification = ['I want', 'I need', 'I will', 'I have', 'I can', \n",
    "                 'I have', 'I should', 'I would', 'I hope', 'I am', \n",
    "                 'I get', 'I might', 'I use', 'I like', 'I take']\n",
    "\n",
    "\n",
    "a = list(itertools.product(texts, justification))\n",
    "combos = []\n",
    "value = []\n",
    "just = []\n",
    "for i in a:\n",
    "    combos.append(i[0]+i[1])\n",
    "    value.append(i[0].split()[0])\n",
    "    just.append(i[1].split()[1])\n",
    "df = pd.DataFrame({'text':combos,\n",
    "                  'value':value,\n",
    "                  'justification':just})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make predictions and save important bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "195/195 [==============================] - 1s     \n"
     ]
    }
   ],
   "source": [
    "test_seq = tokenizer.texts_to_sequences(combos)\n",
    "full_dat = pad_sequences(test_seq, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "preds = model.predict(full_dat, verbose=1)\n",
    "word_index = tokenizer.word_index\n",
    "index_word={}\n",
    "for i,x in word_index.items():\n",
    "    index_word[x]=i\n",
    "np.save('testsentiw_justself.npy', index_word)\n",
    "np.save('testsentiw_inv_justself.npy', word_index)\n",
    "np.save('testsentpreds_justself.npy', preds)\n",
    "np.save('testsentxdat_justself.npy', full_dat)\n",
    "np.save('testdat_justself.npy', df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "overleaf create project, add acl style file and start editing!"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
