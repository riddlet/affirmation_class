{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing word vectors.\n",
      "Found 2196016 word vectors.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "np.random.seed(1337)\n",
    "__docformat__ = 'restructedtext en'\n",
    "import timeit\n",
    "import numpy\n",
    "import scipy.io\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers import Dense, Input, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.models import Model\n",
    "import sys\n",
    "BASE_DIR = '/Users/triddle/Documents/Gits'\n",
    "GLOVE_DIR = BASE_DIR + '/Data/'\n",
    "#MAX_SEQUENCE_LENGTH = 210\n",
    "MAX_NB_WORDS = 20000\n",
    "EMBEDDING_DIM = 300\n",
    "VALIDATION_SPLIT = 0.2\n",
    "# first, build index mapping words in the embeddings set\n",
    "# to their embedding vector\n",
    "\n",
    "print('Indexing word vectors.')\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(GLOVE_DIR, 'glove.840B.300d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for custom metrics\n",
    "import keras.backend as K\n",
    "from keras.utils.generic_utils import get_from_module\n",
    "\n",
    "def categorical_accuracy(y_true, y_pred):\n",
    "    \"\"\"Categorical accuracy metric.\n",
    "\n",
    "    Computes the mean accuracy rate across all predictions for\n",
    "    multiclass classification problems.\n",
    "    \"\"\"\n",
    "    return K.mean(K.equal(K.argmax(y_true, axis=-1),\n",
    "                          K.argmax(y_pred, axis=-1)))\n",
    "\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    \"\"\"Precision metric.\n",
    "\n",
    "    Only computes a batch-wise average of precision.\n",
    "\n",
    "    Computes the precision, a metric for multi-label classification of\n",
    "    how many selected items are relevant.\n",
    "    \"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    \"\"\"Recall metric.\n",
    "\n",
    "    Only computes a batch-wise average of recall.\n",
    "\n",
    "    Computes the recall, a metric for multi-label classification of\n",
    "    how many relevant items are selected.\n",
    "    \"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "\n",
    "def fbeta_score(y_true, y_pred, beta=1):\n",
    "    \"\"\"Computes the F score.\n",
    "\n",
    "    The F score is the weighted harmonic mean of precision and recall.\n",
    "    Here it is only computed as a batch-wise average, not globally.\n",
    "\n",
    "    This is useful for multi-label classification, where input samples can be\n",
    "    classified as sets of labels. By only using accuracy (precision) a model\n",
    "    would achieve a perfect score by simply assigning every class to every\n",
    "    input. In order to avoid this, a metric should penalize incorrect class\n",
    "    assignments as well (recall). The F-beta score (ranged from 0.0 to 1.0)\n",
    "    computes this, as a weighted mean of the proportion of correct class\n",
    "    assignments vs. the proportion of incorrect class assignments.\n",
    "\n",
    "    With beta = 1, this is equivalent to a F-measure. With beta < 1, assigning\n",
    "    correct classes becomes more important, and with beta > 1 the metric is\n",
    "    instead weighted towards penalizing incorrect class assignments.\n",
    "    \"\"\"\n",
    "    if beta < 0:\n",
    "        raise ValueError('The lowest choosable beta is zero (only precision).')\n",
    "\n",
    "    # If there are no true positives, fix the F score at 0 like sklearn.\n",
    "    if K.sum(K.round(K.clip(y_true, 0, 1))) == 0:\n",
    "        return 0\n",
    "\n",
    "    p = precision(y_true, y_pred)\n",
    "    r = recall(y_true, y_pred)\n",
    "    bb = beta ** 2\n",
    "    fbeta_score = (1 + bb) * (p * r) / (bb * p + r + K.epsilon())\n",
    "    return fbeta_score\n",
    "\n",
    "\n",
    "def fmeasure(y_true, y_pred):\n",
    "    \"\"\"Computes the f-measure, the harmonic mean of precision and recall.\n",
    "\n",
    "    Here it is only computed as a batch-wise average, not globally.\n",
    "    \"\"\"\n",
    "    return fbeta_score(y_true, y_pred, beta=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_dat(df_ess, df_dem):\n",
    "    df_ess = df_ess[df_ess.Study=='Connecticut']\n",
    "    df_ess.Condition.replace(['c', 'c2', 'c1', 'c3', 'ca', 'cb', '3'], 'Control', inplace=True)\n",
    "    df_ess.Condition.replace(['t', 't2', 't3', 't1', '1', '2', 'ta', 'tb'], 'Treatment', inplace=True)\n",
    "    df_ess.Condition.replace(['c/t'], np.nan, inplace=True)\n",
    "    \n",
    "    df_dem = df_dem[df_dem.Study=='Connecticut']\n",
    "    df_dem.Ethnicity.replace('Asian', 'Asian American', inplace=True)\n",
    "    df_dem.Ethnicity.replace('Other/Mixed', 'Other', inplace=True)\n",
    "    df_dem = df_dem[['ID', 'Ethnicity', 'Gender']].dropna()\n",
    "    \n",
    "    outdat = pd.merge(df_ess[['ID', 'Intervention_number', 'Essay', 'Condition', 'Intervention_Date', 'corrected']], \n",
    "                    df_dem, how='left', on='ID').drop_duplicates()\n",
    "    \n",
    "    return(outdat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/triddle/anaconda/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing text dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/triddle/anaconda/lib/python2.7/site-packages/pandas/core/generic.py:3430: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._update_inplace(new_data)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6224 texts.\n",
      "Found 5816 unique tokens.\n",
      "Shape of data tensor: (6224, 208)\n",
      "Shape of label tensor: (6224, 4)\n",
      "Shape of xtrain tensor: (5291, 208)\n",
      "Shape of ytrain tensor: (5291, 4)\n",
      "Shape of xval tensor: (933, 208)\n",
      "Shape of yval tensor: (933, 4)\n",
      "Preparing embedding matrix.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib\n",
    "from pandas import DataFrame, read_csv\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd #this is how I usually import pandas\n",
    "import sys #only needed to determine Python version number\n",
    "import matplotlib #only needed to determine Matplotlib version number\n",
    "import numpy as np\n",
    "# second, prepare text samples and their labels\n",
    "print('Processing text dataset')\n",
    "df_ess = pd.read_csv('../../Data/3 CSV Files/essays1.23.16.csv', sep='|')\n",
    "df_dem = pd.read_csv('../../Data/3 CSV Files/demog3.2.16.csv')\n",
    "df = create_dat(df_ess, df_dem)\n",
    "df.dropna(axis=0, subset=['corrected', 'Condition'], inplace=True)\n",
    "texts = []  # list of text samples\n",
    "labels_index = {}  # dictionary mapping label name to numeric id\n",
    "labels = []  # list of label ids\n",
    "labels_index['conm']=0\n",
    "labels_index['conf']=1\n",
    "labels_index['affm']=2\n",
    "labels_index['afff']=3\n",
    "texts = df[\"corrected\"].tolist()\n",
    "labelsType = df[\"Condition\"].tolist()\n",
    "labelsRace = df[\"Gender\"].tolist()\n",
    "labelsCombined=[]\n",
    "for i in range(len(labelsType)):\n",
    "    if labelsType[i]=='Control' and labelsRace[i]=='m':\n",
    "        labelsCombined.append(0)\n",
    "    elif labelsType[i]=='Control' and labelsRace[i]=='f':\n",
    "        labelsCombined.append(1)\n",
    "    elif labelsType[i]=='Treatment' and labelsRace[i]=='m':\n",
    "        labelsCombined.append(2)\n",
    "    elif labelsType[i]=='Treatment' and labelsRace[i]=='f':\n",
    "        labelsCombined.append(3)\n",
    "        \n",
    "print('Found %s texts.' % len(texts))\n",
    "\n",
    "# finally, vectorize the text samples into a 2D integer tensor\n",
    "tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "maxseqval=0\n",
    "for i in range(len(sequences)):\n",
    "    if len(sequences[i])>maxseqval:\n",
    "        maxseqval=len(sequences[i])\n",
    "        \n",
    "MAX_SEQUENCE_LENGTH = maxseqval\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "labels = to_categorical(np.asarray(labelsCombined))\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "# split the data into a training set and a validation set\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "x_train = data\n",
    "y_train = labels\n",
    "\n",
    "# split the data into a training set and a validation set\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "VALIDATION_SPLIT = 0.15\n",
    "nb_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "\n",
    "x_train = data[:-nb_validation_samples]\n",
    "y_train = labels[:-nb_validation_samples]\n",
    "x_val = data[-nb_validation_samples:]\n",
    "y_val = labels[-nb_validation_samples:]\n",
    "\n",
    "textstest=[]\n",
    "textsshuff=[]\n",
    "for i in indices:\n",
    "  textsshuff.append(texts[i])\n",
    "index_word={}\n",
    "for i,x in word_index.items():\n",
    "    index_word[x]=i\n",
    "textstest=textsshuff[-nb_validation_samples:-nb_validation_samples]\n",
    "\n",
    "thefile = open('gendercleantimedistmodeltesttexts.txt', 'w')\n",
    "for item in textstest:\n",
    "  thefile.write(\"%s\\n\" % item)  \n",
    "\n",
    "#np.save('gendercleantimedistmodelxtest.npy', x_test)\n",
    "#np.save('gendercleantimedistmodelytest.npy', y_test)\n",
    "\n",
    "print('Shape of xtrain tensor:', x_train.shape)\n",
    "print('Shape of ytrain tensor:', y_train.shape)\n",
    "print('Shape of xval tensor:', x_val.shape)\n",
    "print('Shape of yval tensor:', y_val.shape)\n",
    "#print('Shape of xtest tensor:', x_test.shape)\n",
    "#print('Shape of ytest tensor:', y_test.shape)\n",
    "\n",
    "print('Preparing embedding matrix.')\n",
    "# prepare embedding matrix\n",
    "nb_words = min(MAX_NB_WORDS, len(word_index))\n",
    "embedding_matrix = np.zeros((nb_words + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i > MAX_NB_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "# load pre-trained word embeddings into an Embedding layer\n",
    "# note that we set trainable = False so as to keep the embeddings fixed\n",
    "embedding_layer = Embedding(nb_words + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_2 (Embedding)          (None, 208, 300)      1745100     embedding_input_1[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)              (None, 208, 300)      0           embedding_2[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                    (None, 50)            70200       dropout_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)              (None, 50)            0           lstm_1[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 4)             204         dropout_2[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 1,815,504\n",
      "Trainable params: 1,815,504\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import load_model\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(nb_words + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=True,dropout=0.2))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(LSTM(50))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(len(labels_index), activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_4 (Embedding)          (None, 208, 300)      1745100     embedding_input_2[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)              (None, 208, 300)      0           embedding_4[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "lstm_4 (LSTM)                    (None, 50)            70200       dropout_5[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)              (None, 50)            0           lstm_4[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "dense_4 (Dense)                  (None, 4)             204         dropout_6[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 1,815,504\n",
      "Trainable params: 1,815,504\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "None\n",
      "Train on 5291 samples, validate on 933 samples\n",
      "Epoch 1/35\n",
      "5291/5291 [==============================] - 34s - loss: 1.2839 - precision: 0.4631 - recall: 0.0571 - fmeasure: 0.0926 - categorical_accuracy: 0.3674 - val_loss: 1.0163 - val_precision: 0.5177 - val_recall: 0.2594 - val_fmeasure: 0.3452 - val_categorical_accuracy: 0.4587\n",
      "Epoch 2/35\n",
      "5291/5291 [==============================] - 34s - loss: 1.0201 - precision: 0.5285 - recall: 0.2839 - fmeasure: 0.3672 - categorical_accuracy: 0.4776 - val_loss: 0.8966 - val_precision: 0.5136 - val_recall: 0.4330 - val_fmeasure: 0.4698 - val_categorical_accuracy: 0.5005\n",
      "Epoch 3/35\n",
      "5291/5291 [==============================] - 34s - loss: 0.9266 - precision: 0.5433 - recall: 0.3699 - fmeasure: 0.4390 - categorical_accuracy: 0.5095 - val_loss: 0.9243 - val_precision: 0.5171 - val_recall: 0.4577 - val_fmeasure: 0.4855 - val_categorical_accuracy: 0.4877\n",
      "Epoch 4/35\n",
      "5291/5291 [==============================] - 34s - loss: 0.8893 - precision: 0.5603 - recall: 0.4190 - fmeasure: 0.4790 - categorical_accuracy: 0.5320 - val_loss: 0.9560 - val_precision: 0.5252 - val_recall: 0.4695 - val_fmeasure: 0.4956 - val_categorical_accuracy: 0.4995\n",
      "Epoch 5/35\n",
      "5291/5291 [==============================] - 34s - loss: 0.8701 - precision: 0.5665 - recall: 0.4406 - fmeasure: 0.4952 - categorical_accuracy: 0.5434 - val_loss: 0.8706 - val_precision: 0.5354 - val_recall: 0.4770 - val_fmeasure: 0.5044 - val_categorical_accuracy: 0.5209\n",
      "Epoch 6/35\n",
      "5291/5291 [==============================] - 35s - loss: 0.8485 - precision: 0.5774 - recall: 0.4625 - fmeasure: 0.5132 - categorical_accuracy: 0.5530 - val_loss: 0.7952 - val_precision: 0.5839 - val_recall: 0.5305 - val_fmeasure: 0.5559 - val_categorical_accuracy: 0.5627\n",
      "Epoch 7/35\n",
      "5291/5291 [==============================] - 33s - loss: 0.8283 - precision: 0.5931 - recall: 0.4944 - fmeasure: 0.5391 - categorical_accuracy: 0.5685 - val_loss: 0.7728 - val_precision: 0.5942 - val_recall: 0.5423 - val_fmeasure: 0.5670 - val_categorical_accuracy: 0.5831\n",
      "Epoch 8/35\n",
      "5291/5291 [==============================] - 35s - loss: 0.7914 - precision: 0.6089 - recall: 0.5135 - fmeasure: 0.5570 - categorical_accuracy: 0.5844 - val_loss: 0.8741 - val_precision: 0.5559 - val_recall: 0.5134 - val_fmeasure: 0.5337 - val_categorical_accuracy: 0.5402\n",
      "Epoch 9/35\n",
      "5291/5291 [==============================] - 33s - loss: 0.7844 - precision: 0.6180 - recall: 0.5273 - fmeasure: 0.5688 - categorical_accuracy: 0.5904 - val_loss: 0.7632 - val_precision: 0.6051 - val_recall: 0.5563 - val_fmeasure: 0.5795 - val_categorical_accuracy: 0.5916\n",
      "Epoch 10/35\n",
      "5291/5291 [==============================] - 34s - loss: 0.7742 - precision: 0.6200 - recall: 0.5409 - fmeasure: 0.5775 - categorical_accuracy: 0.5969 - val_loss: 0.7884 - val_precision: 0.5583 - val_recall: 0.5241 - val_fmeasure: 0.5406 - val_categorical_accuracy: 0.5552\n",
      "Epoch 11/35\n",
      "5291/5291 [==============================] - 35s - loss: 0.7704 - precision: 0.6304 - recall: 0.5417 - fmeasure: 0.5824 - categorical_accuracy: 0.6040 - val_loss: 0.8157 - val_precision: 0.5761 - val_recall: 0.4759 - val_fmeasure: 0.5209 - val_categorical_accuracy: 0.5509\n",
      "Epoch 12/35\n",
      "5291/5291 [==============================] - 33s - loss: 0.7499 - precision: 0.6373 - recall: 0.5557 - fmeasure: 0.5931 - categorical_accuracy: 0.6152 - val_loss: 0.8075 - val_precision: 0.5822 - val_recall: 0.5659 - val_fmeasure: 0.5739 - val_categorical_accuracy: 0.5756\n",
      "Epoch 13/35\n",
      "5291/5291 [==============================] - 33s - loss: 0.7316 - precision: 0.6475 - recall: 0.5812 - fmeasure: 0.6125 - categorical_accuracy: 0.6262 - val_loss: 0.7445 - val_precision: 0.6121 - val_recall: 0.5756 - val_fmeasure: 0.5932 - val_categorical_accuracy: 0.5927\n",
      "Epoch 14/35\n",
      "5291/5291 [==============================] - 34s - loss: 0.7179 - precision: 0.6635 - recall: 0.5980 - fmeasure: 0.6289 - categorical_accuracy: 0.6454 - val_loss: 0.8089 - val_precision: 0.5857 - val_recall: 0.5670 - val_fmeasure: 0.5761 - val_categorical_accuracy: 0.5809\n",
      "Epoch 15/35\n",
      "5291/5291 [==============================] - 32s - loss: 0.7060 - precision: 0.6704 - recall: 0.6071 - fmeasure: 0.6371 - categorical_accuracy: 0.6460 - val_loss: 0.7889 - val_precision: 0.6086 - val_recall: 0.5820 - val_fmeasure: 0.5950 - val_categorical_accuracy: 0.6013\n",
      "Epoch 16/35\n",
      "5291/5291 [==============================] - 35s - loss: 0.6955 - precision: 0.6797 - recall: 0.6173 - fmeasure: 0.6468 - categorical_accuracy: 0.6577 - val_loss: 0.8431 - val_precision: 0.6051 - val_recall: 0.5798 - val_fmeasure: 0.5921 - val_categorical_accuracy: 0.5970\n",
      "Epoch 17/35\n",
      "5291/5291 [==============================] - 34s - loss: 0.6918 - precision: 0.6746 - recall: 0.6184 - fmeasure: 0.6452 - categorical_accuracy: 0.6541 - val_loss: 0.7716 - val_precision: 0.6165 - val_recall: 0.5938 - val_fmeasure: 0.6049 - val_categorical_accuracy: 0.6056\n",
      "Epoch 18/35\n",
      "5291/5291 [==============================] - 33s - loss: 0.6817 - precision: 0.6866 - recall: 0.6284 - fmeasure: 0.6561 - categorical_accuracy: 0.6638 - val_loss: 0.7784 - val_precision: 0.6159 - val_recall: 0.5906 - val_fmeasure: 0.6029 - val_categorical_accuracy: 0.6034\n",
      "Epoch 19/35\n",
      "5291/5291 [==============================] - 34s - loss: 0.6803 - precision: 0.6926 - recall: 0.6432 - fmeasure: 0.6669 - categorical_accuracy: 0.6745 - val_loss: 0.7555 - val_precision: 0.6236 - val_recall: 0.5959 - val_fmeasure: 0.6094 - val_categorical_accuracy: 0.6141\n",
      "Epoch 20/35\n",
      "5291/5291 [==============================] - 34s - loss: 0.6632 - precision: 0.6956 - recall: 0.6473 - fmeasure: 0.6704 - categorical_accuracy: 0.6770 - val_loss: 0.7885 - val_precision: 0.6150 - val_recall: 0.5916 - val_fmeasure: 0.6030 - val_categorical_accuracy: 0.6045\n",
      "Epoch 21/35\n",
      "5291/5291 [==============================] - 34s - loss: 0.6577 - precision: 0.7020 - recall: 0.6502 - fmeasure: 0.6749 - categorical_accuracy: 0.6825 - val_loss: 0.7521 - val_precision: 0.6230 - val_recall: 0.6013 - val_fmeasure: 0.6119 - val_categorical_accuracy: 0.6120\n",
      "Epoch 22/35\n",
      "5291/5291 [==============================] - 34s - loss: 0.6441 - precision: 0.7131 - recall: 0.6645 - fmeasure: 0.6878 - categorical_accuracy: 0.6974 - val_loss: 0.8866 - val_precision: 0.6076 - val_recall: 0.5852 - val_fmeasure: 0.5961 - val_categorical_accuracy: 0.6002\n",
      "Epoch 23/35\n",
      "5291/5291 [==============================] - 33s - loss: 0.6461 - precision: 0.7153 - recall: 0.6715 - fmeasure: 0.6926 - categorical_accuracy: 0.6995 - val_loss: 0.7769 - val_precision: 0.6406 - val_recall: 0.6227 - val_fmeasure: 0.6315 - val_categorical_accuracy: 0.6324\n",
      "Epoch 24/35\n",
      "5291/5291 [==============================] - 33s - loss: 0.6271 - precision: 0.7156 - recall: 0.6706 - fmeasure: 0.6923 - categorical_accuracy: 0.7010 - val_loss: 0.7810 - val_precision: 0.6354 - val_recall: 0.6077 - val_fmeasure: 0.6211 - val_categorical_accuracy: 0.6238\n",
      "Epoch 25/35\n",
      "5291/5291 [==============================] - 33s - loss: 0.6229 - precision: 0.7180 - recall: 0.6744 - fmeasure: 0.6954 - categorical_accuracy: 0.7029 - val_loss: 0.7293 - val_precision: 0.6413 - val_recall: 0.6141 - val_fmeasure: 0.6273 - val_categorical_accuracy: 0.6281\n",
      "Epoch 26/35\n",
      "5291/5291 [==============================] - 34s - loss: 0.6089 - precision: 0.7336 - recall: 0.6936 - fmeasure: 0.7130 - categorical_accuracy: 0.7188 - val_loss: 0.7296 - val_precision: 0.6484 - val_recall: 0.6238 - val_fmeasure: 0.6358 - val_categorical_accuracy: 0.6409\n",
      "Epoch 27/35\n",
      "5291/5291 [==============================] - 34s - loss: 0.5975 - precision: 0.7395 - recall: 0.7016 - fmeasure: 0.7199 - categorical_accuracy: 0.7233 - val_loss: 0.7692 - val_precision: 0.6293 - val_recall: 0.6099 - val_fmeasure: 0.6194 - val_categorical_accuracy: 0.6249\n",
      "Epoch 28/35\n",
      "5291/5291 [==============================] - 34s - loss: 0.5988 - precision: 0.7453 - recall: 0.7063 - fmeasure: 0.7251 - categorical_accuracy: 0.7331 - val_loss: 0.7969 - val_precision: 0.6100 - val_recall: 0.5981 - val_fmeasure: 0.6039 - val_categorical_accuracy: 0.6066\n",
      "Epoch 29/35\n",
      "5291/5291 [==============================] - 36s - loss: 0.5867 - precision: 0.7433 - recall: 0.7042 - fmeasure: 0.7231 - categorical_accuracy: 0.7284 - val_loss: 0.8138 - val_precision: 0.6205 - val_recall: 0.6077 - val_fmeasure: 0.6140 - val_categorical_accuracy: 0.6131\n",
      "Epoch 30/35\n",
      "5291/5291 [==============================] - 33s - loss: 0.5732 - precision: 0.7582 - recall: 0.7165 - fmeasure: 0.7367 - categorical_accuracy: 0.7435 - val_loss: 0.8641 - val_precision: 0.6154 - val_recall: 0.6013 - val_fmeasure: 0.6082 - val_categorical_accuracy: 0.6120\n",
      "Epoch 31/35\n",
      "5291/5291 [==============================] - 32s - loss: 0.5590 - precision: 0.7627 - recall: 0.7303 - fmeasure: 0.7461 - categorical_accuracy: 0.7507 - val_loss: 0.8147 - val_precision: 0.6287 - val_recall: 0.6109 - val_fmeasure: 0.6197 - val_categorical_accuracy: 0.6227\n",
      "Epoch 32/35\n",
      "5291/5291 [==============================] - 33s - loss: 0.5649 - precision: 0.7577 - recall: 0.7235 - fmeasure: 0.7401 - categorical_accuracy: 0.7437 - val_loss: 0.8198 - val_precision: 0.6351 - val_recall: 0.6195 - val_fmeasure: 0.6272 - val_categorical_accuracy: 0.6281\n",
      "Epoch 33/35\n",
      "5291/5291 [==============================] - 32s - loss: 0.5459 - precision: 0.7758 - recall: 0.7454 - fmeasure: 0.7603 - categorical_accuracy: 0.7607 - val_loss: 0.8255 - val_precision: 0.6357 - val_recall: 0.6206 - val_fmeasure: 0.6280 - val_categorical_accuracy: 0.6302\n",
      "Epoch 34/35\n",
      "5291/5291 [==============================] - 33s - loss: 0.5530 - precision: 0.7655 - recall: 0.7322 - fmeasure: 0.7484 - categorical_accuracy: 0.7507 - val_loss: 0.8133 - val_precision: 0.6300 - val_recall: 0.6088 - val_fmeasure: 0.6192 - val_categorical_accuracy: 0.6174\n",
      "Epoch 35/35\n",
      "5291/5291 [==============================] - 33s - loss: 0.5276 - precision: 0.7841 - recall: 0.7515 - fmeasure: 0.7674 - categorical_accuracy: 0.7689 - val_loss: 0.8591 - val_precision: 0.6203 - val_recall: 0.6002 - val_fmeasure: 0.6101 - val_categorical_accuracy: 0.6077\n"
     ]
    }
   ],
   "source": [
    "#model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=[precision,recall,fmeasure,categorical_accuracy])\n",
    "#print(model.summary())\n",
    "#bz=128\n",
    "#model.fit(x_train, y_train, validation_data=(x_val, y_val),\n",
    "#          nb_epoch=35, batch_size=bz)\n",
    "#model.save_weights('finalgendermodel.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_2 (Embedding)          (None, 208, 300)      1745100     embedding_input_1[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)              (None, 208, 300)      0           embedding_2[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                    (None, 50)            70200       dropout_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)              (None, 50)            0           lstm_1[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 4)             204         dropout_2[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 1,815,504\n",
      "Trainable params: 1,815,504\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['dropout_1', 'lstm_1', 'dense_1', 'dropout_2', 'embedding_2']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import TimeDistributed\n",
    "model.load_weights('finalgendermodel.h5')\n",
    "model.summary()\n",
    "layer_dict = dict([(layer.name, layer) for layer in model.layers])\n",
    "layer_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_2 (Embedding)          (None, 208, 300)      1745100     embedding_input_1[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)              (None, 208, 300)      0           embedding_2[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                    (None, 208, 50)       70200       dropout_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)              (None, 208, 50)       0           lstm_2[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "timedistributed_1 (TimeDistribut (None, 208, 4)        204         dropout_3[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 1,815,504\n",
      "Trainable params: 1,815,504\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#layer_dict['lstm_2'].get_weights()\n",
    "dens=layer_dict['dense_1'].get_weights()\n",
    "lstmw=layer_dict['lstm_1'].get_weights()\n",
    "model.pop()\n",
    "model.pop()\n",
    "model.pop()\n",
    "lstmout = LSTM(50,\n",
    "               return_sequences=True,\n",
    "               stateful=False,weights=lstmw)\n",
    "model.add(lstmout)\n",
    "model.add(Dropout(0.5))\n",
    "templayer=TimeDistributed(Dense(len(labels_index), activation='softmax',weights=dens))\n",
    "model.add(templayer)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.25946808,  0.2427309 ,  0.26948729,  0.22831374],\n",
       "       [ 0.2520169 ,  0.2447765 ,  0.28034008,  0.22286652],\n",
       "       [ 0.24652748,  0.24267758,  0.29390815,  0.2168868 ],\n",
       "       [ 0.24286158,  0.23721558,  0.30969056,  0.21023227],\n",
       "       [ 0.24073376,  0.22947262,  0.32675067,  0.20304295],\n",
       "       [ 0.23983349,  0.22049777,  0.34406051,  0.19560823],\n",
       "       [ 0.23988432,  0.21115462,  0.36070469,  0.18825635],\n",
       "       [ 0.24066298,  0.20207317,  0.37598497,  0.1812789 ],\n",
       "       [ 0.24199603,  0.19365996,  0.38945323,  0.17489077],\n",
       "       [ 0.24374832,  0.18613701,  0.40089476,  0.16921991],\n",
       "       [ 0.24581099,  0.17958942,  0.41028357,  0.16431604],\n",
       "       [ 0.24809258,  0.17400992,  0.41772857,  0.16016893],\n",
       "       [ 0.25051424,  0.1693352 ,  0.42342219,  0.1567284 ],\n",
       "       [ 0.25300723,  0.16547269,  0.42759848,  0.1539216 ],\n",
       "       [ 0.25551242,  0.16231835,  0.43050307,  0.15166618],\n",
       "       [ 0.25797984,  0.15976788,  0.43237305,  0.14987922],\n",
       "       [ 0.26036873,  0.15772317,  0.43342561,  0.1484825 ],\n",
       "       [ 0.26264706,  0.15609536,  0.43385205,  0.14740555],\n",
       "       [ 0.26479086,  0.15480633,  0.43381611,  0.14658669],\n",
       "       [ 0.26678351,  0.1537887 ,  0.43345451,  0.14597327],\n",
       "       [ 0.2686148 ,  0.15298547,  0.43287861,  0.14552115],\n",
       "       [ 0.27028   ,  0.15234917,  0.43217689,  0.14519395],\n",
       "       [ 0.27177891,  0.15184093,  0.43141809,  0.14496209],\n",
       "       [ 0.27311495,  0.1514294 ,  0.43065378,  0.14480187],\n",
       "       [ 0.2742945 ,  0.15108982,  0.42992112,  0.14469458],\n",
       "       [ 0.27532607,  0.15080304,  0.42924532,  0.14462556],\n",
       "       [ 0.2762197 ,  0.15055454,  0.42864215,  0.14458361],\n",
       "       [ 0.27698639,  0.15033364,  0.42811975,  0.14456023],\n",
       "       [ 0.27763772,  0.15013283,  0.42768034,  0.14454912],\n",
       "       [ 0.27818537,  0.14994693,  0.42732206,  0.14454564],\n",
       "       [ 0.27864099,  0.14977264,  0.42703977,  0.14454658],\n",
       "       [ 0.27901563,  0.14960811,  0.42682657,  0.14454971],\n",
       "       [ 0.27931991,  0.14945237,  0.42667416,  0.14455354],\n",
       "       [ 0.27956375,  0.14930515,  0.4265739 ,  0.14455719],\n",
       "       [ 0.27975616,  0.14916652,  0.42651713,  0.14456017],\n",
       "       [ 0.27990538,  0.14903678,  0.42649558,  0.14456226],\n",
       "       [ 0.28001878,  0.1489163 ,  0.42650151,  0.14456341],\n",
       "       [ 0.28010285,  0.14880534,  0.4265281 ,  0.1445637 ],\n",
       "       [ 0.28016323,  0.1487041 ,  0.42656934,  0.14456332],\n",
       "       [ 0.2802048 ,  0.14861262,  0.42662019,  0.14456239],\n",
       "       [ 0.28023165,  0.14853078,  0.42667642,  0.14456113],\n",
       "       [ 0.28024736,  0.14845833,  0.42673469,  0.14455962],\n",
       "       [ 0.28025472,  0.14839484,  0.42679238,  0.14455807],\n",
       "       [ 0.28025612,  0.14833987,  0.42684746,  0.14455655],\n",
       "       [ 0.28025338,  0.14829277,  0.42689872,  0.14455512],\n",
       "       [ 0.28024805,  0.14825284,  0.42694527,  0.14455386],\n",
       "       [ 0.28024119,  0.14821947,  0.42698655,  0.14455277],\n",
       "       [ 0.28023371,  0.1481919 ,  0.42702252,  0.14455189],\n",
       "       [ 0.28022614,  0.14816947,  0.42705315,  0.1445512 ],\n",
       "       [ 0.28021899,  0.14815147,  0.42707884,  0.14455068],\n",
       "       [ 0.28021252,  0.14813729,  0.42709988,  0.14455032],\n",
       "       [ 0.28020689,  0.1481263 ,  0.42711672,  0.14455011],\n",
       "       [ 0.28020209,  0.14811799,  0.42712992,  0.14454998],\n",
       "       [ 0.28019819,  0.14811186,  0.42714   ,  0.14454997],\n",
       "       [ 0.28019515,  0.14810751,  0.42714736,  0.14454998],\n",
       "       [ 0.28019285,  0.14810456,  0.42715254,  0.14455004],\n",
       "       [ 0.28019127,  0.14810269,  0.42715591,  0.14455013],\n",
       "       [ 0.28019026,  0.14810164,  0.42715788,  0.14455023],\n",
       "       [ 0.28018969,  0.14810118,  0.4271588 ,  0.14455032],\n",
       "       [ 0.28018954,  0.14810115,  0.42715889,  0.14455041],\n",
       "       [ 0.28018969,  0.1481014 ,  0.42715845,  0.14455047],\n",
       "       [ 0.28019002,  0.14810181,  0.42715764,  0.14455052],\n",
       "       [ 0.28019056,  0.1481023 ,  0.42715663,  0.14455053],\n",
       "       [ 0.28019118,  0.14810282,  0.42715546,  0.14455052],\n",
       "       [ 0.28019181,  0.14810333,  0.42715433,  0.14455052],\n",
       "       [ 0.28019246,  0.1481038 ,  0.42715326,  0.14455047],\n",
       "       [ 0.28019312,  0.14810421,  0.42715225,  0.14455044],\n",
       "       [ 0.28019372,  0.14810453,  0.42715138,  0.14455037],\n",
       "       [ 0.28019428,  0.1481048 ,  0.42715061,  0.14455032],\n",
       "       [ 0.28019476,  0.148105  ,  0.42714998,  0.14455025],\n",
       "       [ 0.28019521,  0.14810513,  0.4271495 ,  0.14455017],\n",
       "       [ 0.28019559,  0.14810523,  0.42714906,  0.14455011],\n",
       "       [ 0.28019586,  0.14810525,  0.42714885,  0.14455004],\n",
       "       [ 0.28019613,  0.14810526,  0.42714864,  0.14454997],\n",
       "       [ 0.28019634,  0.14810523,  0.42714849,  0.14454992],\n",
       "       [ 0.28019649,  0.14810517,  0.42714849,  0.14454986],\n",
       "       [ 0.28019658,  0.14810511,  0.42714849,  0.14454979],\n",
       "       [ 0.28019667,  0.14810504,  0.42714855,  0.14454976],\n",
       "       [ 0.28019673,  0.14810498,  0.42714858,  0.14454971],\n",
       "       [ 0.28019676,  0.14810489,  0.42714867,  0.14454967],\n",
       "       [ 0.28019676,  0.14810482,  0.42714876,  0.14454967],\n",
       "       [ 0.28019676,  0.14810476,  0.42714888,  0.14454962],\n",
       "       [ 0.28019676,  0.14810467,  0.42714897,  0.14454961],\n",
       "       [ 0.28019673,  0.14810462,  0.42714906,  0.14454959],\n",
       "       [ 0.2801967 ,  0.14810458,  0.42714915,  0.14454959],\n",
       "       [ 0.28019667,  0.14810453,  0.42714921,  0.14454958],\n",
       "       [ 0.28019667,  0.1481045 ,  0.42714927,  0.14454956],\n",
       "       [ 0.28019664,  0.14810449,  0.42714933,  0.14454956],\n",
       "       [ 0.28019664,  0.14810444,  0.42714936,  0.14454956],\n",
       "       [ 0.28019658,  0.14810443,  0.42714942,  0.14454955],\n",
       "       [ 0.28019658,  0.14810441,  0.42714944,  0.14454955],\n",
       "       [ 0.28019658,  0.14810441,  0.42714944,  0.14454955],\n",
       "       [ 0.28019655,  0.1481044 ,  0.4271495 ,  0.14454955],\n",
       "       [ 0.28019658,  0.1481044 ,  0.42714947,  0.14454953],\n",
       "       [ 0.28019658,  0.14810438,  0.4271495 ,  0.14454953],\n",
       "       [ 0.28019658,  0.14810438,  0.4271495 ,  0.14454955],\n",
       "       [ 0.28019658,  0.14810438,  0.4271495 ,  0.14454953],\n",
       "       [ 0.28019658,  0.14810438,  0.4271495 ,  0.14454955],\n",
       "       [ 0.28019658,  0.14810438,  0.4271495 ,  0.14454955],\n",
       "       [ 0.28019658,  0.1481044 ,  0.42714947,  0.14454953],\n",
       "       [ 0.28019658,  0.1481044 ,  0.42714947,  0.14454953],\n",
       "       [ 0.28019658,  0.14810441,  0.42714947,  0.14454956],\n",
       "       [ 0.28019658,  0.14810438,  0.42714947,  0.14454956],\n",
       "       [ 0.28019658,  0.14810441,  0.42714947,  0.14454956],\n",
       "       [ 0.28019658,  0.14810441,  0.42714944,  0.14454955],\n",
       "       [ 0.28019658,  0.14810441,  0.42714944,  0.14454955],\n",
       "       [ 0.28019658,  0.14810441,  0.42714944,  0.14454955],\n",
       "       [ 0.28019658,  0.14810441,  0.42714944,  0.14454955],\n",
       "       [ 0.28019658,  0.14810441,  0.42714944,  0.14454955],\n",
       "       [ 0.28019658,  0.14810441,  0.42714947,  0.14454953],\n",
       "       [ 0.28019658,  0.14810441,  0.42714947,  0.14454953],\n",
       "       [ 0.28019658,  0.14810441,  0.42714944,  0.14454955],\n",
       "       [ 0.28019658,  0.14810441,  0.42714947,  0.14454956],\n",
       "       [ 0.28019658,  0.14810441,  0.42714947,  0.14454953],\n",
       "       [ 0.28019658,  0.14810441,  0.42714944,  0.14454955],\n",
       "       [ 0.28019658,  0.14810441,  0.42714944,  0.14454955],\n",
       "       [ 0.28019658,  0.14810441,  0.42714944,  0.14454955],\n",
       "       [ 0.28019658,  0.14810441,  0.42714944,  0.14454955],\n",
       "       [ 0.28019658,  0.14810441,  0.42714944,  0.14454955],\n",
       "       [ 0.28019658,  0.14810441,  0.42714944,  0.14454955],\n",
       "       [ 0.28019658,  0.14810441,  0.42714944,  0.14454955],\n",
       "       [ 0.28019658,  0.14810441,  0.42714944,  0.14454955],\n",
       "       [ 0.28019658,  0.14810441,  0.42714944,  0.14454955],\n",
       "       [ 0.28019658,  0.14810441,  0.42714944,  0.14454955],\n",
       "       [ 0.28019661,  0.14810441,  0.42714944,  0.14454956],\n",
       "       [ 0.28019661,  0.14810441,  0.42714944,  0.14454956],\n",
       "       [ 0.28019661,  0.14810441,  0.42714944,  0.14454956],\n",
       "       [ 0.28019661,  0.14810441,  0.42714944,  0.14454956],\n",
       "       [ 0.28019661,  0.14810441,  0.42714944,  0.14454956],\n",
       "       [ 0.28019661,  0.14810441,  0.42714944,  0.14454956],\n",
       "       [ 0.28019661,  0.14810441,  0.42714944,  0.14454956],\n",
       "       [ 0.28019661,  0.14810441,  0.42714944,  0.14454956],\n",
       "       [ 0.28019661,  0.14810441,  0.42714944,  0.14454956],\n",
       "       [ 0.28019661,  0.14810441,  0.42714944,  0.14454956],\n",
       "       [ 0.28019661,  0.14810441,  0.42714944,  0.14454956],\n",
       "       [ 0.28019661,  0.14810441,  0.42714944,  0.14454956],\n",
       "       [ 0.28019661,  0.14810441,  0.42714944,  0.14454956],\n",
       "       [ 0.28019661,  0.14810441,  0.42714944,  0.14454956],\n",
       "       [ 0.28019661,  0.14810441,  0.42714944,  0.14454956],\n",
       "       [ 0.28019661,  0.14810441,  0.42714944,  0.14454956],\n",
       "       [ 0.28019661,  0.14810441,  0.42714944,  0.14454956],\n",
       "       [ 0.28019661,  0.14810441,  0.42714944,  0.14454956],\n",
       "       [ 0.28019661,  0.14810441,  0.42714944,  0.14454956],\n",
       "       [ 0.28019661,  0.14810441,  0.42714944,  0.14454956],\n",
       "       [ 0.28019661,  0.14810441,  0.42714944,  0.14454956],\n",
       "       [ 0.28019661,  0.14810441,  0.42714944,  0.14454956],\n",
       "       [ 0.28019661,  0.14810441,  0.42714944,  0.14454956],\n",
       "       [ 0.28019661,  0.14810441,  0.42714944,  0.14454956],\n",
       "       [ 0.28019661,  0.14810441,  0.42714944,  0.14454956],\n",
       "       [ 0.28019661,  0.14810441,  0.42714944,  0.14454956],\n",
       "       [ 0.28019661,  0.14810441,  0.42714944,  0.14454956],\n",
       "       [ 0.28019661,  0.14810441,  0.42714944,  0.14454956],\n",
       "       [ 0.28019661,  0.14810441,  0.42714944,  0.14454956],\n",
       "       [ 0.28019661,  0.14810441,  0.42714944,  0.14454956],\n",
       "       [ 0.28019661,  0.14810441,  0.42714944,  0.14454956],\n",
       "       [ 0.28019661,  0.14810441,  0.42714944,  0.14454956],\n",
       "       [ 0.28019661,  0.14810441,  0.42714944,  0.14454956],\n",
       "       [ 0.28019661,  0.14810441,  0.42714944,  0.14454956],\n",
       "       [ 0.28019661,  0.14810441,  0.42714944,  0.14454956],\n",
       "       [ 0.28019661,  0.14810441,  0.42714944,  0.14454956],\n",
       "       [ 0.28019661,  0.14810441,  0.42714944,  0.14454956],\n",
       "       [ 0.28019661,  0.14810441,  0.42714944,  0.14454956],\n",
       "       [ 0.28019661,  0.14810441,  0.42714944,  0.14454956],\n",
       "       [ 0.28019661,  0.14810441,  0.42714944,  0.14454956],\n",
       "       [ 0.28019661,  0.14810441,  0.42714944,  0.14454956],\n",
       "       [ 0.28019661,  0.14810441,  0.42714944,  0.14454956],\n",
       "       [ 0.28019661,  0.14810441,  0.42714944,  0.14454956],\n",
       "       [ 0.28019661,  0.14810441,  0.42714944,  0.14454956],\n",
       "       [ 0.28019661,  0.14810441,  0.42714944,  0.14454956],\n",
       "       [ 0.28019661,  0.14810441,  0.42714944,  0.14454956],\n",
       "       [ 0.28019661,  0.14810441,  0.42714944,  0.14454956],\n",
       "       [ 0.28019661,  0.14810441,  0.42714944,  0.14454956],\n",
       "       [ 0.28019661,  0.14810441,  0.42714944,  0.14454956],\n",
       "       [ 0.32082367,  0.165497  ,  0.40150547,  0.11217387],\n",
       "       [ 0.28108522,  0.15377161,  0.46085379,  0.10428938],\n",
       "       [ 0.26853877,  0.12669714,  0.49856004,  0.10620406],\n",
       "       [ 0.33043763,  0.17488463,  0.42054558,  0.07413216],\n",
       "       [ 0.46534789,  0.21612106,  0.24675569,  0.07177536],\n",
       "       [ 0.4756915 ,  0.24736612,  0.22636439,  0.050578  ],\n",
       "       [ 0.48041767,  0.27667388,  0.20317425,  0.0397342 ],\n",
       "       [ 0.50164574,  0.25059509,  0.21120653,  0.03655263],\n",
       "       [ 0.53050083,  0.30943817,  0.1328523 ,  0.02720872],\n",
       "       [ 0.58969688,  0.30520493,  0.07959761,  0.02550061],\n",
       "       [ 0.56667298,  0.34759161,  0.06730419,  0.01843126],\n",
       "       [ 0.55012655,  0.39050758,  0.0462537 ,  0.01311216],\n",
       "       [ 0.58245867,  0.35964429,  0.04629229,  0.01160474],\n",
       "       [ 0.5367136 ,  0.43211108,  0.02251736,  0.00865796],\n",
       "       [ 0.5637688 ,  0.40737766,  0.0188009 ,  0.01005264],\n",
       "       [ 0.5124023 ,  0.46317822,  0.01662141,  0.00779806],\n",
       "       [ 0.46608412,  0.51852244,  0.01010938,  0.00528403],\n",
       "       [ 0.5089339 ,  0.47506282,  0.01143052,  0.00457276],\n",
       "       [ 0.42533219,  0.56549346,  0.00563104,  0.00354333],\n",
       "       [ 0.47140804,  0.51607174,  0.00715701,  0.00536324],\n",
       "       [ 0.40686637,  0.58093083,  0.00747207,  0.00473072],\n",
       "       [ 0.35999408,  0.63215894,  0.00467203,  0.00317496],\n",
       "       [ 0.41430169,  0.57696491,  0.00602352,  0.00270986],\n",
       "       [ 0.32881844,  0.66582322,  0.00315045,  0.00220793],\n",
       "       [ 0.39682385,  0.59440941,  0.00488859,  0.00387813],\n",
       "       [ 0.32869577,  0.66200185,  0.00559278,  0.00370958],\n",
       "       [ 0.29064938,  0.7033999 ,  0.00350451,  0.00244621],\n",
       "       [ 0.34726787,  0.64592546,  0.00474947,  0.0020572 ],\n",
       "       [ 0.26686668,  0.72894692,  0.00249668,  0.00168971],\n",
       "       [ 0.34601301,  0.6465286 ,  0.00421394,  0.00324445],\n",
       "       [ 0.27847803,  0.71323526,  0.00503683,  0.00324987],\n",
       "       [ 0.24726221,  0.74748993,  0.00312643,  0.00212144],\n",
       "       [ 0.30216604,  0.69175082,  0.0043136 ,  0.00176952],\n",
       "       [ 0.22679719,  0.76952779,  0.00224386,  0.00143119],\n",
       "       [ 0.31172571,  0.68143302,  0.00392834,  0.00291293]], dtype=float32)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "#sequences = tokenizer.texts_to_sequences(texts)\n",
    "#pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "testdat = ['These values might', 'These values are', 'These values will', 'Sports']\n",
    "test_seq = tokenizer.texts_to_sequences(testdat)\n",
    "full_dat = pad_sequences(test_seq, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "preds = model.predict(full_dat, verbose=1)\n",
    "preds[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../Text_dictionaries/LIWC/Social.csv', header=None, names=['word'], encoding='UTF-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>pos</th>\n",
       "      <th>lemma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>he</td>\n",
       "      <td>PRON</td>\n",
       "      <td>he</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>hed</td>\n",
       "      <td>PRON</td>\n",
       "      <td>-PRON-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>he'd</td>\n",
       "      <td>PRON</td>\n",
       "      <td>-PRON-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>he'll</td>\n",
       "      <td>PRON</td>\n",
       "      <td>-PRON-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>herself</td>\n",
       "      <td>PRON</td>\n",
       "      <td>herself</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>hes</td>\n",
       "      <td>PRON</td>\n",
       "      <td>-PRON-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>he's</td>\n",
       "      <td>PRON</td>\n",
       "      <td>-PRON-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>him</td>\n",
       "      <td>PRON</td>\n",
       "      <td>him</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>himself</td>\n",
       "      <td>PRON</td>\n",
       "      <td>himself</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>ourselves</td>\n",
       "      <td>PRON</td>\n",
       "      <td>ourselves</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344</th>\n",
       "      <td>she</td>\n",
       "      <td>PRON</td>\n",
       "      <td>she</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>she'd</td>\n",
       "      <td>PRON</td>\n",
       "      <td>-PRON-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346</th>\n",
       "      <td>she'll</td>\n",
       "      <td>PRON</td>\n",
       "      <td>-PRON-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>shes</td>\n",
       "      <td>PRON</td>\n",
       "      <td>-PRON-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>she's</td>\n",
       "      <td>PRON</td>\n",
       "      <td>-PRON-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386</th>\n",
       "      <td>thee</td>\n",
       "      <td>PRON</td>\n",
       "      <td>thee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388</th>\n",
       "      <td>them</td>\n",
       "      <td>PRON</td>\n",
       "      <td>them</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389</th>\n",
       "      <td>themselves</td>\n",
       "      <td>PRON</td>\n",
       "      <td>themselves</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390</th>\n",
       "      <td>they</td>\n",
       "      <td>PRON</td>\n",
       "      <td>they</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391</th>\n",
       "      <td>theyd</td>\n",
       "      <td>PRON</td>\n",
       "      <td>-PRON-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392</th>\n",
       "      <td>they'd</td>\n",
       "      <td>PRON</td>\n",
       "      <td>-PRON-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393</th>\n",
       "      <td>theyll</td>\n",
       "      <td>PRON</td>\n",
       "      <td>-PRON-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394</th>\n",
       "      <td>they'll</td>\n",
       "      <td>PRON</td>\n",
       "      <td>-PRON-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>theyre</td>\n",
       "      <td>PRON</td>\n",
       "      <td>-PRON-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>they're</td>\n",
       "      <td>PRON</td>\n",
       "      <td>-PRON-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>theyve</td>\n",
       "      <td>PRON</td>\n",
       "      <td>-PRON-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>they've</td>\n",
       "      <td>PRON</td>\n",
       "      <td>-PRON-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409</th>\n",
       "      <td>us</td>\n",
       "      <td>PRON</td>\n",
       "      <td>us</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>we</td>\n",
       "      <td>PRON</td>\n",
       "      <td>we</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412</th>\n",
       "      <td>wed</td>\n",
       "      <td>PRON</td>\n",
       "      <td>-PRON-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>we'd</td>\n",
       "      <td>PRON</td>\n",
       "      <td>-PRON-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>we'll</td>\n",
       "      <td>PRON</td>\n",
       "      <td>-PRON-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418</th>\n",
       "      <td>we're</td>\n",
       "      <td>PRON</td>\n",
       "      <td>-PRON-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>419</th>\n",
       "      <td>weve</td>\n",
       "      <td>PRON</td>\n",
       "      <td>-PRON-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420</th>\n",
       "      <td>we've</td>\n",
       "      <td>PRON</td>\n",
       "      <td>-PRON-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>441</th>\n",
       "      <td>ya</td>\n",
       "      <td>PRON</td>\n",
       "      <td>ya</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>442</th>\n",
       "      <td>yall</td>\n",
       "      <td>PRON</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>443</th>\n",
       "      <td>y'all</td>\n",
       "      <td>PRON</td>\n",
       "      <td>y'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>445</th>\n",
       "      <td>you</td>\n",
       "      <td>PRON</td>\n",
       "      <td>you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>446</th>\n",
       "      <td>youd</td>\n",
       "      <td>PRON</td>\n",
       "      <td>-PRON-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>447</th>\n",
       "      <td>you'd</td>\n",
       "      <td>PRON</td>\n",
       "      <td>-PRON-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>448</th>\n",
       "      <td>youll</td>\n",
       "      <td>PRON</td>\n",
       "      <td>-PRON-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449</th>\n",
       "      <td>you'll</td>\n",
       "      <td>PRON</td>\n",
       "      <td>-PRON-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>451</th>\n",
       "      <td>youre</td>\n",
       "      <td>PRON</td>\n",
       "      <td>-PRON-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>452</th>\n",
       "      <td>you're</td>\n",
       "      <td>PRON</td>\n",
       "      <td>-PRON-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>454</th>\n",
       "      <td>youve</td>\n",
       "      <td>PRON</td>\n",
       "      <td>-PRON-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>455</th>\n",
       "      <td>you've</td>\n",
       "      <td>PRON</td>\n",
       "      <td>-PRON-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           word   pos       lemma\n",
       "169          he  PRON          he\n",
       "174         hed  PRON      -PRON-\n",
       "175        he'd  PRON      -PRON-\n",
       "176       he'll  PRON      -PRON-\n",
       "185     herself  PRON     herself\n",
       "186         hes  PRON      -PRON-\n",
       "187        he's  PRON      -PRON-\n",
       "190         him  PRON         him\n",
       "191     himself  PRON     himself\n",
       "287   ourselves  PRON   ourselves\n",
       "344         she  PRON         she\n",
       "345       she'd  PRON      -PRON-\n",
       "346      she'll  PRON      -PRON-\n",
       "347        shes  PRON      -PRON-\n",
       "348       she's  PRON      -PRON-\n",
       "386        thee  PRON        thee\n",
       "388        them  PRON        them\n",
       "389  themselves  PRON  themselves\n",
       "390        they  PRON        they\n",
       "391       theyd  PRON      -PRON-\n",
       "392      they'd  PRON      -PRON-\n",
       "393      theyll  PRON      -PRON-\n",
       "394     they'll  PRON      -PRON-\n",
       "395      theyre  PRON      -PRON-\n",
       "396     they're  PRON      -PRON-\n",
       "397      theyve  PRON      -PRON-\n",
       "398     they've  PRON      -PRON-\n",
       "409          us  PRON          us\n",
       "411          we  PRON          we\n",
       "412         wed  PRON      -PRON-\n",
       "413        we'd  PRON      -PRON-\n",
       "417       we'll  PRON      -PRON-\n",
       "418       we're  PRON      -PRON-\n",
       "419        weve  PRON      -PRON-\n",
       "420       we've  PRON      -PRON-\n",
       "441          ya  PRON          ya\n",
       "442        yall  PRON           y\n",
       "443       y'all  PRON          y'\n",
       "445         you  PRON         you\n",
       "446        youd  PRON      -PRON-\n",
       "447       you'd  PRON      -PRON-\n",
       "448       youll  PRON      -PRON-\n",
       "449      you'll  PRON      -PRON-\n",
       "451       youre  PRON      -PRON-\n",
       "452      you're  PRON      -PRON-\n",
       "454       youve  PRON      -PRON-\n",
       "455      you've  PRON      -PRON-"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df[df.word.str.contains('\\*')]\n",
    "df['pos'] = df['word'].apply(lambda x: nlp(x)[0].pos_)\n",
    "df['lemma'] = df['word'].apply(lambda x: nlp(x)[0].lemma_)\n",
    "df.head(10)\n",
    "#doc = nlp(df.word.loc[0])\n",
    "#doc[0].pos_\n",
    "#df[df.pos=='NOUN']['sent'] = 'These values are important because my'\n",
    "#df[df.pos=='VERB']['sent'] = 'These values'\n",
    "df[df.pos=='PRON'] = 'These values are importnat '\n",
    "#ADJ\n",
    "#INTJ\n",
    "#ADV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outgendernewviz=model.predict(x_test,verbose=1)\n",
    "np.save('finalgenderdict.npy', index_word) \n",
    "np.save('finalgenderdictinv.npy', word_index) \n",
    "np.save('finalgenderoutput.npy', outgendernewviz) \n",
    "np.save('finalgenderxtestdata.npy', x_test) \n",
    "np.save('finalgenderytestdata.npy', y_test) \n",
    "thefile = open('finalgendertextsinput.txt', 'w')\n",
    "for item in textstest:\n",
    "  thefile.write(\"%s\\n\" % item) "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
